> library(MASS)       # Pima.tr / Pima.te
> library(class)      # k-NN
> library(rpart)      # decision tree
> library(rpart.plot) # tree plot
> library(kernlab)      # SVM
> library(pROC)       # ROC/AUC
> 
> set.seed(42)
> 
> train <- Pima.tr
> test  <- Pima.te
> 
> # 目的変数（2値）
> table(train$type)

 No Yes 
132  68 
> table(test$type)

 No Yes 
223 109 
> 
> # 説明変数（数値）と目的変数
> x_cols <- setdiff(names(train), "type")
> 
> 
> eval_binary <- function(y_true, y_pred, positive = levels(y_true)[2]) {
+   # y_true, y_pred は factor を想定
+   y_true <- factor(y_true)
+   y_pred <- factor(y_pred, levels = levels(y_true))
+   
+   tab <- table(True = y_true, Pred = y_pred)
+   
+   # 指標計算
+   pos <- positive
+   neg <- setdiff(levels(y_true), pos)[1]
+   
+   TP <- tab[pos, pos]
+   TN <- tab[neg, neg]
+   FP <- tab[neg, pos]
+   FN <- tab[pos, neg]
+   
+   acc <- (TP + TN) / sum(tab)
+   prec <- if ((TP + FP) == 0) NA else TP / (TP + FP)
+   rec  <- if ((TP + FN) == 0) NA else TP / (TP + FN)
+   f1   <- if (is.na(prec) || is.na(rec) || (prec + rec) == 0) NA else 2 * prec * rec / (prec + rec)
+   
+   list(
+     confusion = tab,
+     metrics = data.frame(
+       accuracy = acc,
+       precision = prec,
+       recall = rec,
+       f1 = f1,
+       row.names = NULL
+     )
+   )
+ }
> 
> # ROC/AUC
> eval_roc <- function(y_true, prob_pos, positive = levels(y_true)[2]) {
+   roc_obj <- pROC::roc(y_true, prob_pos, levels = rev(levels(y_true)))
+   auc_val <- as.numeric(pROC::auc(roc_obj))
+   list(roc = roc_obj, auc = auc_val)
+ }
> 
> # 比較用
> results <- list()
> 
> # 標準化
> standardize_train_test <- function(train_df, test_df, cols) {
+   mu <- sapply(train_df[, cols, drop = FALSE], mean)
+   sdv <- sapply(train_df[, cols, drop = FALSE], sd)
+   train_z <- as.data.frame(scale(train_df[, cols, drop = FALSE], center = mu, scale = sdv))
+   test_z  <- as.data.frame(scale(test_df[, cols, drop = FALSE],  center = mu, scale = sdv))
+   list(train_z = train_z, test_z = test_z)
+ }
> 
> # =========================
> # 2) ロジスティック回帰
> # =========================
> glm_fit <- glm(type ~ ., data = train, family = binomial())
> 
> # 予測
> glm_prob <- predict(glm_fit, newdata = test, type = "response")
> glm_pred <- ifelse(glm_prob >= 0.5, levels(train$type)[2], levels(train$type)[1])
> glm_pred <- factor(glm_pred, levels = levels(train$type))
> 
> results$logistic <- eval_binary(test$type, glm_pred)
> 
> # ROC/AUC
> glm_roc <- eval_roc(test$type, glm_prob)
Setting direction: controls > cases
> results$logistic$auc <- glm_roc$auc
> 
> # 係数
> glm_coef <- sort(coef(glm_fit), decreasing = TRUE)
> glm_coef
         ped        npreg          bmi          age          glu 
 1.820410367  0.103183427  0.083623912  0.041183529  0.032116823 
        skin           bp  (Intercept) 
-0.001916632 -0.004767542 -9.773061533 
> 
> results$logistic
$confusion
     Pred
True   No Yes
  No  200  23
  Yes  43  66

$metrics
   accuracy precision    recall        f1
1 0.8012048  0.741573 0.6055046 0.6666667

$auc
[1] 0.8658823

> 
> # =========================
> # 3) k-NN（kを変えて比較）
> # =========================
> # kNNは距離ベースなので標準化が
> z <- standardize_train_test(train, test, x_cols)
> train_z <- z$train_z
> test_z  <- z$test_z
> 
> k_list <- seq(1, 40, by = 2)
> 
> knn_metrics <- data.frame()
> knn_confusions <- list()
> 
> for (k in k_list) {
+   knn_pred <- knn(
+     train = train_z,
+     test  = test_z,
+     cl    = train$type,
+     k     = k
+   )
+   ev <- eval_binary(test$type, knn_pred)
+   knn_confusions[[paste0("k=", k)]] <- ev$confusion
+   knn_metrics <- rbind(
+     knn_metrics,
+     data.frame(model = paste0("kNN(k=", k, ")"), ev$metrics)
+   )
+ }
> 
> results$knn <- list(metrics_table = knn_metrics, confusion_list = knn_confusions)
> results$knn
$metrics_table
       model  accuracy precision    recall        f1
1   kNN(k=1) 0.7048193 0.5523810 0.5321101 0.5420561
2   kNN(k=3) 0.7409639 0.6263736 0.5229358 0.5700000
3   kNN(k=5) 0.7439759 0.6395349 0.5045872 0.5641026
4   kNN(k=7) 0.7530120 0.6666667 0.4954128 0.5684211
5   kNN(k=9) 0.7650602 0.6962025 0.5045872 0.5851064
6  kNN(k=11) 0.7650602 0.7012987 0.4954128 0.5806452
7  kNN(k=13) 0.7650602 0.7012987 0.4954128 0.5806452
8  kNN(k=15) 0.7710843 0.7088608 0.5137615 0.5957447
9  kNN(k=17) 0.7620482 0.7142857 0.4587156 0.5586592
10 kNN(k=19) 0.7590361 0.7042254 0.4587156 0.5555556
11 kNN(k=21) 0.7680723 0.7222222 0.4770642 0.5745856
12 kNN(k=23) 0.7801205 0.7571429 0.4862385 0.5921788
13 kNN(k=25) 0.7801205 0.7571429 0.4862385 0.5921788
14 kNN(k=27) 0.7771084 0.7536232 0.4770642 0.5842697
15 kNN(k=29) 0.7740964 0.7428571 0.4770642 0.5810056
16 kNN(k=31) 0.7801205 0.7571429 0.4862385 0.5921788
17 kNN(k=33) 0.7771084 0.7611940 0.4678899 0.5795455
18 kNN(k=35) 0.7801205 0.7727273 0.4678899 0.5828571
19 kNN(k=37) 0.7771084 0.7611940 0.4678899 0.5795455
20 kNN(k=39) 0.7740964 0.7575758 0.4587156 0.5714286

$confusion_list
$confusion_list$`k=1`
     Pred
True   No Yes
  No  176  47
  Yes  51  58

$confusion_list$`k=3`
     Pred
True   No Yes
  No  189  34
  Yes  52  57

$confusion_list$`k=5`
     Pred
True   No Yes
  No  192  31
  Yes  54  55

$confusion_list$`k=7`
     Pred
True   No Yes
  No  196  27
  Yes  55  54

$confusion_list$`k=9`
     Pred
True   No Yes
  No  199  24
  Yes  54  55

$confusion_list$`k=11`
     Pred
True   No Yes
  No  200  23
  Yes  55  54

$confusion_list$`k=13`
     Pred
True   No Yes
  No  200  23
  Yes  55  54

$confusion_list$`k=15`
     Pred
True   No Yes
  No  200  23
  Yes  53  56

$confusion_list$`k=17`
     Pred
True   No Yes
  No  203  20
  Yes  59  50

$confusion_list$`k=19`
     Pred
True   No Yes
  No  202  21
  Yes  59  50

$confusion_list$`k=21`
     Pred
True   No Yes
  No  203  20
  Yes  57  52

$confusion_list$`k=23`
     Pred
True   No Yes
  No  206  17
  Yes  56  53

$confusion_list$`k=25`
     Pred
True   No Yes
  No  206  17
  Yes  56  53

$confusion_list$`k=27`
     Pred
True   No Yes
  No  206  17
  Yes  57  52

$confusion_list$`k=29`
     Pred
True   No Yes
  No  205  18
  Yes  57  52

$confusion_list$`k=31`
     Pred
True   No Yes
  No  206  17
  Yes  56  53

$confusion_list$`k=33`
     Pred
True   No Yes
  No  207  16
  Yes  58  51

$confusion_list$`k=35`
     Pred
True   No Yes
  No  208  15
  Yes  58  51

$confusion_list$`k=37`
     Pred
True   No Yes
  No  207  16
  Yes  58  51

$confusion_list$`k=39`
     Pred
True   No Yes
  No  207  16
  Yes  59  50


> 
> # kごとの精度比較（図）
> acc <- knn_metrics$accuracy
> 
> ylim <- range(acc) + c(-0.01, 0.01)
> plot(k_list, acc,
+      type = "o", pch = 1, lwd = 0.5,
+      xlab = "k (odd values)", ylab = "Accuracy",
+      ylim = ylim,
+      main = "k-NN: Accuracy vs k (Pima)")
> grid(nx = NA, ny = NULL, lty = 2)
> 
> 
> # =========================
> # 4) 決定木（rpart）＋剪定
> # =========================
> tree_fit <- rpart(type ~ ., data = train, method = "class")
> 
> # 交差検証結果
> printcp(tree_fit)

Classification tree:
rpart(formula = type ~ ., data = train, method = "class")

Variables actually used in tree construction:
[1] age bmi bp  glu ped

Root node error: 68/200 = 0.34

n= 200 

        CP nsplit rel error  xerror     xstd
1 0.220588      0   1.00000 1.00000 0.098518
2 0.161765      1   0.77941 1.05882 0.099827
3 0.073529      2   0.61765 0.77941 0.091785
4 0.058824      3   0.54412 0.80882 0.092863
5 0.014706      4   0.48529 0.75000 0.090647
6 0.010000      7   0.44118 0.88235 0.095305
> 
> cp_tbl <- tree_fit$cptable
> min_i <- which.min(cp_tbl[, "xerror"])
> min_cp <- cp_tbl[min_i, "CP"]
> min_xerr <- cp_tbl[min_i, "xerror"]
> 
> # 1-SE ルール（最小xerror + 標準誤差）
> xerr_1se <- min_xerr + cp_tbl[min_i, "xstd"]
> i_1se <- which(cp_tbl[, "xerror"] <= xerr_1se)[1]   # 最初に閾値を下回る（=より単純な木）
> cp_1se <- cp_tbl[i_1se, "CP"]
> 
> #cpプロット
> plotcp(tree_fit,
+        main = "Cost-Complexity Pruning (rpart)",
+        lwd = 1.5, cex = 1)
> 
> grid(nx = NA, ny = NULL, lty = 3)
> 
> # 1-SE の水平線
> abline(h = xerr_1se, lty = 2)
> 
> # 最小点と1-SE点を強調
> points(min_i, cp_tbl[min_i, "xerror"], pch = 19, cex = 1.2)
> points(i_1se, cp_tbl[i_1se, "xerror"], pch = 17, cex = 1.2)
> 
> legend("topright",
+        legend = c(
+          paste0("min xerror CP=", signif(min_cp, 3)),
+          paste0("1-SE CP=", signif(cp_1se, 3)),
+          "1-SE threshold"
+        ),
+        pch = c(19, 17, NA),
+        lty = c(NA, NA, 2),
+        bty = "n")
> 
> # もっともCV誤差が小さい cp を採用して剪定
> best_cp <- tree_fit$cptable[which.min(tree_fit$cptable[, "xerror"]), "CP"]
> tree_pruned <- prune(tree_fit, cp = best_cp)
> 
> # 予測（剪定前 / 剪定後）
> tree_pred <- predict(tree_fit, newdata = test, type = "class")
> prun_pred <- predict(tree_pruned, newdata = test, type = "class")
> 
> results$tree_raw    <- eval_binary(test$type, tree_pred)
> results$tree_pruned <- eval_binary(test$type, prun_pred)
> 
> # 木の可視化
> rpart.plot(tree_fit, main = "Decision Tree (raw)")
> rpart.plot(tree_pruned, main = "Decision Tree (pruned)")
> 
> # =========================
> # 5) SVM
> # =========================
> # SVMも標準化した入力を使用（train_z/test_z を使う）
> svm_train <- cbind(train_z, type = train$type)
> svm_test  <- cbind(test_z,  type = test$type)
> 
> # kernlab::ksvm
> # prob.model=TRUE を付けると確率が取れる
> svm_fit <- ksvm(
+   type ~ .,
+   data = svm_train,
+   type = "C-svc",
+   kernel = "rbfdot",
+   C = 1,
+   prob.model = TRUE
+ )
> 
> # 予測クラス
> svm_pred <- predict(svm_fit, newdata = svm_test[, x_cols], type = "response")
> results$svm <- eval_binary(test$type, svm_pred)
> 
> # 確率（ROC/AUC 用）
> # predict(..., type="probabilities") でクラスごとの確率行列が返る
> svm_prob_mat <- predict(svm_fit, newdata = svm_test[, x_cols], type = "probabilities")
> 
> # 「陽性クラス」を results の定義と合わせる（levels(train$type)[2] を陽性扱い）
> pos_class <- levels(train$type)[2]
> svm_prob <- svm_prob_mat[, pos_class]
> 
> # ROC/AUC
> svm_roc <- eval_roc(test$type, svm_prob)
Setting direction: controls > cases
> results$svm$auc <- svm_roc$auc
> 
> results$svm
$confusion
     Pred
True   No Yes
  No  198  25
  Yes  50  59

$metrics
   accuracy precision    recall       f1
1 0.7740964  0.702381 0.5412844 0.611399

$auc
[1] 0.831818

> 
> # =========================
> # 6) 結果まとめ
> # =========================
> summary_table <- rbind(
+   data.frame(model = "Logistic", results$logistic$metrics, AUC = results$logistic$auc),
+   data.frame(model = "Tree(raw)", results$tree_raw$metrics, AUC = NA),
+   data.frame(model = "Tree(pruned)", results$tree_pruned$metrics, AUC = NA),
+   data.frame(model = "SVM(RBF)", results$svm$metrics, AUC = results$svm$auc)
+ )
> summary_table
         model  accuracy precision    recall        f1       AUC
1     Logistic 0.8012048 0.7415730 0.6055046 0.6666667 0.8658823
2    Tree(raw) 0.7319277 0.5980392 0.5596330 0.5781991        NA
3 Tree(pruned) 0.7560241 0.6590909 0.5321101 0.5888325        NA
4     SVM(RBF) 0.7740964 0.7023810 0.5412844 0.6113990 0.8318180
> 
> # 混同行列を表示
> results$logistic$confusion
     Pred
True   No Yes
  No  200  23
  Yes  43  66
> results$tree_raw$confusion
     Pred
True   No Yes
  No  182  41
  Yes  48  61
> results$tree_pruned$confusion
     Pred
True   No Yes
  No  193  30
  Yes  51  58
> results$svm$confusion
     Pred
True   No Yes
  No  198  25
  Yes  50  59
> 
> # ROC曲線を同じ図に重ねる
> plot(glm_roc$roc,
+      main = "ROC curves (Logistic vs SVM)",
+      col = "blue",
+      lwd = 2,
+      print.auc = TRUE)
> 
> plot(svm_roc$roc,
+      add = TRUE,
+      col = "red",
+      lwd = 2,
+      print.auc = TRUE,
+      print.auc.y = 0.4)
> 
> legend("bottomright",
+        legend = c(
+          paste0("Logistic (AUC=", round(results$logistic$auc, 3), ")"),
+          paste0("SVM (AUC=", round(results$svm$auc, 3), ")")
+        ),
+        col = c("blue", "red"),
+        lwd = 2,
+        bty = "n")